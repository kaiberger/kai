
@InProceedings{RBL+10,
  Title                    = {{Caroline: An Autonomously Driving Vehicle for Urban Environments}},
  Author                   = {Rauskolb, Fred W. and Berger, Kai and Lipski, Christian and Magnor, Marcus and Cornelsen, Karsten and Effertz, Jan and Form, Thomas and Graefe, Fabian and Ohl, Sebastian and Schumacher, Walter and Wille, J\"{o}rn-Marten and Hecker, Peter and Nothdurft, Tobias and Doering, Michael and Homeier, Kai and Morgenroth, Johannes and Wolf, Lars and Basarke, Christian and Berger, Christian and G\"{u}lke, Tim and Klose, Felix and Rumpe, Bernhard},
  Booktitle                = {The DARPA Urban Challenge},
  Year                     = {2009},

  Address                  = {Berlin Heidelberg},
  Editor                   = {Buehler, Martin and Iagnemma, Karl and Singh, Sanjiv},
  Month                    = nov,
  Pages                    = {441--508},
  Publisher                = {Springer-Verlag},

  Abstract                 = {The 2007 DARPA Urban Challenge afforded the golden opportunity for the Technische Universitaet Braunschweig to demonstrate its abilities to develop an autonomously driving vehicle to compete with the world's best competitors. After several stages of qualification, our team CarOLO qualified early for the DARPA Urban Challenge Final Event and was among only eleven teams from initially 89 competitors to compete in the final. We had the ability to work together in a large group of experts, each contributing his expertise in his discipline, and significant organisational, financial and technical support by local sponsors who helped us to become the best non-US team. In this report, we describe the 2007 DARPA Urban Challenge, our contribution "Caroline", the technology and algorithms along with her performance in the DARPA Urban Challenge Final Event on November 3, 2007.},
  Doi                      = {10.1007/978-3-642-03991-1\_11},
  File                     = {:RelatedWork/2009 - Rauskolb et al. - Caroline An Autonomously Driving Vehicle for Urban Environments.pdf:pdf},
  keywords                 = {EN,MINE,bookchapter,reviewed},
  Url                      = {https://arxiv.org/pdf/1409.6584}
}

@inproceedings{LSB+08,
  title={A fast and robust approach to lane marking detection and lane tracking},
  author={Lipski, Christian and Scholz, Bj\"{o}rn and Berger, Kai and Linz, Christian and Stich, Timo and Magnor, Marcus},
  booktitle={IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)},
  abstract={We present a lane detection algorithm that robustly detects and tracks various lane markings in real-time. The first part is a feature detection algorithm that transforms several input images into a top view perspective and analyzes local histograms. For this part we make use of state-of-the-art graphics hardware. The second part fits a very simple and flexible lane model to these lane marking features. The algorithm was thoroughly tested on an autonomous vehicle that was one of the finalists in the 2007 DARPA Urban Challenge. In combination with other sensors, i.e. a lidar, radar and vision based obstacle detection and surface classification, the autonomous vehicle is able to drive in an urban scenario at up to 15 mp/h.},
  pages={57--60},
  year={2008},
  keywords={MINE,EN,conference,reviewed},
  url={http://ieeexplore.ieee.org/document/4512284/},
  organization={IEEE}
}

@inproceedings{LLB+10,
  title={Virtual Video Camera: Image-Based Viewpoint Navigation Through Space and Time},
  author={Lipski, Christian and Linz, Christian and Berger, Kai and Sellent, Anita and Magnor, Marcus},
  booktitle={Computer Graphics Forum},
  volume={29},
  number={8},
  pages={2555--2568},
  year={2010},
  keywords={MINE,EN,conference,reviewed},
  abstract={We present an image-based rendering system to viewpoint-navigate through space and time of complex real-world, dynamic scenes. Our approach accepts unsynchronized, uncalibrated multivideo footage as input. Inexpensive, consumer-grade camcorders suffice to acquire arbitrary scenes, for example in the outdoors, without elaborate recording setup procedures, allowing also for hand-held recordings. Instead of scene depth estimation, layer segmentation or 3D reconstruction, our approach is based on dense image correspondences, treating view interpolation uniformly in space and time: spatial viewpoint navigation, slow motion or freeze-and-rotate effects can all be created in the same way. Acquisition simplification, integration of moving cameras, generalization to difficult scenes and space-time symmetric interpolation amount to a widely applicable virtual video camera system.},
  url={http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2010.01824.x/pdf},
  organization={Wiley Online Library}
}

@inproceedings{BRS+11,
  title={Markerless motion capture using multiple color-depth sensors},
  author={Berger, Kai and Ruhl, Kai and Schroeder, Yannic and Bruemmer, Christian and Scholz, Alexander and Magnor, Marcus},
  abstract={With the advent of the Microsoft Kinect, renewed focus has been put on monocular depth-based motion capturing. However, this approach is limited in that an actor has to move facing the camera. Due to the active light nature of the sensor, no more than one device has been used for motion capturing so far. In effect, any pose estimation must fail for poses occluded to the depth camera. Our work investigates on reducing or mitigating the detrimental effects of multiple active light emitters, thereby allowing motion capture from all angles. We systematically evaluate the concurrent use of one to four Kinects, including calibration, error measures and analysis, and present a time-multiplexing approach.},
  booktitle={VMV},
  pages={317--324},
  keywords={MINE,EN,conference,reviewed},
  year={2011},
  url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.9939&rep=rep1&type=pdf}
}

@article{SSB+11,
  title={Multiple kinect studies},
  author={Schr\"{o}der, Yannic and Scholz, Alexander and Berger, Kai and Ruhl, Kai and Guthe, Stefan and Magnor, Marcus},
  abstract={This project investigates multi-camera setups using Microsoft Kinects. Active structured light from the Kinect is used in several scenarious, including gas flow description, motion capture and free-viewpoint video. While the ability to capture depth alongside color data (RGB-D) is the starting point of the investigations, the structured light is also used more directly. In order to combine Kinects with passive recording approaches, common calibration with HD cameras is also a topic.},
  journal={Computer Graphics},
  volume={2},
  number={4},
  pages={6},
  keywords={MINE,EN,techreport,reviewed},
  url={https://pdfs.semanticscholar.org/8b58/fce6a54b76ffa1f571d184ec558364c0740e.pdf},
  year={2011}
}

@inproceedings{BLL+08,
  title={The area processing unit of Caroline-Finding the way through DARPA's Urban Challenge},
  author={Berger, Kai and Lipski, Christian and Linz, Christian and Stich, Timo and Magnor, Marcus},
  booktitle={International Workshop on Robot Vision},
  abstract={This paper presents a vision-based color segmentation algorithm suitable for urban environments that separates an image into areas of drivable and non-drivable terrain. Assuming that a part of the image is known to be drivable terrain, other parts of the image are classified by comparing the Euclidean distance of each pixel's color to the mean colors of the drivable area in real-time. Moving the search area depending on each frame's result ensures temporal consistency and coherence. Furthermore, the algorithm classifies artifacts such as white and yellow lane markings and hard shadows as areas of unknown drivability. The algorithm was thoroughly tested on the autonomous vehicle 'Caroline', which was a finalist in the 2007 DARPA Urban Challenge.},
  pages={260--274},
  year={2008},
  keywords={MINE,EN,conference,reviewed},
  organization={Springer, Berlin, Heidelberg},
  url={https://pdfs.semanticscholar.org/4b9d/b808c06635b784ce6c1409603c0487bcd684.pdf}
}

@article{BBB+08,
  title={Team CarOLO--Technical Paper},
  author={Basarke, Christian and Berger, Christian and Berger, Kai and Cornelsen, Karsten and Doering, Michael and Effertz, Jan and Form, Thomas and G\"{u}lke, Tim and Graefe, Fabian and Hecker, Peter and others},
  journal={Technische Universit\"{a}t Braunschweig, Braunschweig, Germany, Informatik-Bericht},
  volume={7},
  keywords={MINE,EN,techreport,reviewed},
  year={2008}
}

@inproceedings{BIA+09,
  title={Tomographic 4D Reconstruction of Gas Flows in the Presence of Occluders},
  author={Berger, Kai and Ihrke, Ivo and Atcheson, Bradley and Heidrich, Wolfgang and Magnor, Marcus A and others},
  abstract={We present a method that allows for reconstructing non-stationary, time-varying gas flows around moving objects. Our work extends the background oriented Schlieren tomography (3D-BOS) acquisition technique to capture gas flows also in the presence of occluding objects. An algorithm is presented that exploits the unique properties of BOS background patterns to robustly segment occluding objects. Numerical issues in the refractive index field reconstruction are addressed and successfully solved by the new method.},
  booktitle={VMV},
  pages={29--36},
  keywords={MINE,EN,conference,reviewed},
  url={https://pdfs.semanticscholar.org/15ea/77d4a88cccc0bb4a43e68e53084fa7ead1c9.pdf},
  year={2009}
}

@inproceedings{BLL+10a,
  title={A ghosting artifact detector for interpolated image quality assessment},
  author={Berger, Kai and Lipski, Christian and Linz, Christian and Sellent, Anita and Magnor, Marcus},
  abstract={We present a no-reference image quality metric for image interpolation. The approach is capable of detecting blurry regions as well as ghosting artifacts, e.g., in image based rendering scenarios. Based on the assumption that ghosting artifacts can be detected locally, perceived visual quality can be predicted from the amount of regions that are affected by ghosting. Because the approach does not require any reference image, it is very suitable, e.g., for assessing quality of image-based rendering techniques in general settings.},
  booktitle={IEEE 14th International Symposium on Consumer Electronics (ISCE)},
  pages={1--6},
  year={2010},
  keywords={MINE,EN,conference,reviewed},
  url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.1659&rep=rep1&type=pdf},
  organization={IEEE}
}

@article{BWWM12,
  title={Modeling and verifying the polarizing reflectance of real-world metallic surfaces},
  author={Berger, Kai and Weidlich, Andrea and Wilkie, Alexander and Magnor, Marcus},
  abstract={Using measurements of real-world samples of metals, the proposed approach verifies predictions of bidirectional reflectance distribution function (BRDF) models. It employs ellipsometry to verify both the actual polarizing effect and the overall reflectance behavior of the metallic surfaces.},
  journal={IEEE computer graphics and applications},
  volume={32},
  number={2},
  pages={24--33},
  year={2012},
  keywords={MINE,EN,journal,reviewed},
  url={https://www.cg.cs.tu-bs.de/upload/publications/cgaBerger.pdf},
  publisher={IEEE}
}

@article{LBM09,
  title={vIsage-A visualization and debugging framework for distributed system applications},
  author={Lipski, Christian and Berger, Kai and Magnor, Marcus},
  abstract={We present a Visualization, Simulation, And Graphical debugging Environment (vIsage) for distributed systems. Time-varying spatial data as well as other information from different sources can be displayed and superimposed in a single view at run-time. The main contribution of our framework is that it is not just a tool for visualizing the data, but it is a graphical interface for a simulation environment. Real world data can be recorded, played back or even synthesized. This enables testing and debugging of single components of complex distributed systems. Being the missing link between development, simulation and testing, e.g., in robotics applications, it was designed to significantly increase the efficiency of the software development process.},
  url={https://pdfs.semanticscholar.org/6a6b/a9c3c1f6979dc3bf59adfb7876b79f26b6be.pdf},
  year={2009},
  keywords={MINE,EN,conference,reviewed},
  publisher={V\'{a}clav Skala-UNION Agency}
}

@article{KABM10,
  title={Real-time gaze tracking with a consumer-grade video camera},
  author={Keil, Andrea and Albuquerque, Georgia and Berger, Kai and Magnor, Marcus},
  abstract={Eye gaze can be a rich source of information to identify particular interests of human users. Eye gaze tracking has been largely used in different research areas in the last years, as for example in psychology, visual system design and to leverage the user interaction with computer systems. In this paper, we present an IR-based gaze tracking framework that can be easily coupled to common user applications and allows for real-time gaze estimation. Compared to other gaze tracking systems, our system uses only affordable consumer-grade hardware and still achieves fair accuracy. To evaluate the usability of our gaze tracking system, we performed a user study with persons of different genders and ethnicities.},
  year={2010},
  keywords={MINE,EN,conference,reviewed},
  url={https://goo.gl/kNcZaS},
  publisher={V\'{a}clav Skala-UNION Agency}
}

@article{LBE+10,
  title={Sparse bundle adjustment speedup strategies},
  author={Lipski, Christian and Bose, Denis and Eisemann, Martin and Berger, Kai and Magnor, Marcus},
  abstract={Over the past years, Structure-from-Motion calibration algorithms have become widely popular for many applications in computer graphics. From an unordered set of photographs, they manage to robustly estimate intrinsic and extrinsic camera parameters for each image. One major drawback is the quadratic computation time of existing algorithms. This paper presents different strategies to overcome this problem by only working on subsets of images and merging the results. A quantitative comparison of these strategies reveals the trade-off between accuracy and computation time.},
  year={2010},
  keywords={MINE,EN,conference,reviewed},
  url={https://otik.uk.zcu.cz/bitstream/11025/11045/1/Lipski.pdf},
  publisher={V\'{a}clav Skala-UNION Agency}
}

@inproceedings{RLK+11,
  title={Integrating multiple depth sensors into the virtual video camera},
  author={Ruhl, Kai and Berger, Kai and Lipski, Christian and Klose, Felix and Schroeder, Yannic and Scholz, Alexander and Magnor, Marcus},
  booktitle={ACM SIGGRAPH 2011 Posters},
  abstract={In this ongoing work, we present our efforts to incorporate depth sensors into a multi camera system for free view-point video. Both the video cameras and the depth sensors are consumer grade. Our free-viewpoint system, the Virtual Video Camera, uses image-based rendering to create novel views between widely spaced (up to 15 degrees) cameras, using dense image correspondences. The introduction of multiple depth sensors into the system allows us to obtain approximate depth information for many pixels, thereby providing a valuable hint for estimating pixel correspondences between cameras.},
  pages={23},
  year={2011},
  url={https://graphics.tu-bs.de/upload/publications/kinectVVC.pdf},
  keywords={MINE,EN,conference,reviewed},
  organization={ACM}
}

@article{IBA+09,
  title={Tomographic reconstruction and efficient rendering of refractive gas flows},
  author={Ihrke, Ivo and Berger, Kai and Atcheson, Bradley and Magnor, Marcus and Heidrich, Wolfgang},
  abstract={This chapter introduces techniques for the capture and efficient display of dynamic three-dimensional non-stationary gas flows. We describe a flexible Schlieren-tomographic system consisting of multiple consumer camcorders. A special choice of background pattern for Background Oriented Schlieren (BOS) imaging provides for flexibility in the experimental setup. Optical flow techniques are used to measure image space deflections due to heated air flows from arbitrary camera positions. A specially tailored sparse-view algebraic reconstruction algorithm is employed to tomographically recover a refractive index gradient field. After robust integration of these gradient fields, time-varying, fully three-dimensional refractive index fields are obtained. These can be rendered efficiently using a ray-casting style algorithm that is suitable for graphics hardware acceleration. Additional optical properties can be rendered within the same computational framework.},
  journal={Imaging Measurement Methods for Flow Analysis},
  pages={145--154},
  year={2009},
  url={http://vccimaging.org/Publications/Ihrke2009TRE/Ihrke2009TRE.pdf},
  keywords={MINE,EN,bookchapter,reviewed},
  publisher={Springer Berlin/Heidelberg}
}

@article{BLL+10b,
  title={Target Space Modeling-The End of 3D Widgets},
  author={Berger, Kai and Linz, Christian and Lipski, Christian and Vaudrey, Tobi and Klette, Reinhard and Magnor, Marcus},
  year={2010},
  abstract={In today's modeling tools, the graphical user interfaces are required to be accurate and intuitive to use. Most tools therefor rely on additional 3D-widgets (e.g., arrows or circles) that enable the user to operate towards a desired modeling result. In this paper we present, for the first time, a method that makes these widgets obsolete. We propose to use simple geometric primitives such as planes or spheres as low-dimensional subspaces, so called target spaces for the interaction. Instead of operating towards a modeling result, the user then directly steers the result. The target spaces suffice to be indicated to the user just as additional visual information. We verify by means of a user study that with our method it is now possible to develop accurate single-view GUIs without 3D-widgets that are highly intuitive to use.},
  keywords={MINE,EN,conference,reviewed},
  url={http://hdl.handle.net/11025/11034},
  publisher={V\'{a}clav Skala-UNION Agency}
}

@article{BWWM+12,
  title={Verifying and Modeling the Polarizing Reflectance of Real-World Metallic Surfaces},
  author={Berger, Kai and Weidlich, Andrea and Wilkie, Alexander and Magnor, Marcus},
  abstract={Using measurements of real-world samples of metals, the proposed approach verifies predictions of bidirectional reflectance distribution function (BRDF) models. It employs ellipsometry to verify both the actual polarizing effect and the overall reflectance behavior of the metallic surfaces.},
  keywords={MINE,EN,techreport,reviewed},
  url={http://ieeexplore.ieee.org/document/6109209/},
  year={2012}
}

@inproceedings{BRMI11a,
  title={Refractive index dependent bidirectional scattering distribution functions},
  abstract={We investigate the effect of immersing real-world materials into media of different refractive indices. We show, only some materials follow the Fresnel-governed behaviour. In reality, many materials exhibit unexpected effects such as stronger localized highlights or a significant increase in the glossy reflection due to microgeometry. We propose a parametric model that, however, takes their Fresnel-governed behaviour into account.},
  author={Berger, Kai and Reshetouski, Ilya and Magnor, Marcus and Ihrke, Ivo},
  booktitle={ACM SIGGRAPH 2011 Posters},
  pages={66},
  year={2011},
  url={https://www.cg.cs.tu-bs.de/upload/publications/riBRDf.pdf},
  keywords={MINE,EN,conference,reviewed},
  organization={ACM}
}

@inproceedings{BRMI11b,
  title={Measuring BRDFs of Immersed Materials},
  author={Berger, Kai and Reshetouski, Ilya and Magnor, Marcus A and Ihrke, Ivo},
  abstract={We investigate the effect of immersing real-world materials into media of different refractive indices. We show, that only some materials follow the Fresnel-governed behaviour. In reality, many materials exhibit unexpected effects such as stronger localized highlights or a significant increase in the glossy reflection due to microgeometry. In this paper, we propose a new measurement technique that allows for measuring the BRDFs of materials that are immersed into different media.},
  url={http://www.ilya.o-x-t.com/publications/vmv11.pdf},
  booktitle={VMV},
  pages={325--330},
  keywords={MINE,EN,conference,reviewed},
  year={2011}
}

@article{BLL+09,
  title={Echtzeiterkennung von befahrbaren Bereichen in urbanen Szenarien},
  author={Berger, Kai and Linz, Christian and Lipski, Christian and Stich, Timo and Magnor, Marcus},
  journal={Aktuelle Anwendungen in Technik und Wirtschaft},
  pages={1--10},
  year={2009},
  keywords={MINE,EN,conference,reviewed},
  publisher={Springer Berlin Heidelberg}
}

@article{BSGK13,
  title={Using sparse optical flow for multiple Kinect applications},
  author={Berger, Kai and Schroeder, Yannic and Guthe, Stefan and Kastner, Marc Aurel},
  abstract={The use of Multiple Microsoft Kinects has become prominent in the last two years and enjoyed widespread acceptance. While several work has been published to mitigate quality degradations in the precomputed depth image, this work focuses on employing an optical flow suitable for dot patterns as employed in the Kinect to retrieve subtle scene data alterations for reconstruction. The method is employed in a multiple Kinect vision architecture to detect the interface of propane flow around occluding objects in air.},
  url={https://pdfs.semanticscholar.org/9a73/2add677a3bb26b6b9f5cac5ddafdb42d7ca2.pdf},
  year={2013},
  keywords={MINE,EN,conference,reviewed}
}

@inproceedings{CBA+13,
  title={SHREC'13 Track: Retrieval on Textured 3D Models},
  author={Cerri, Andrea and Biasotti, Silvia and Abdelrahman, Mostafa and Angulo, Jes\'{u}s and Berger, Kai and Chevallier, Louis and El-Melegy, Moumen T and Farag, Aly A and Lefebvre, Frederic and Giachetti, Andrea and others},
  abstract={This contribution reports the results of the SHREC 2013 track: Retrieval on Textured 3D Models, whose goal is to evaluate the performance of retrieval algorithms when models vary either by geometric shape or texture, or both. The collection to search in is made of 240 textured mesh models, divided into 10 classes. Each model has been used in turn as a query against the remaining part of the database. For a given query, the goal was to retrieve the most similar objects. The track saw six participants and the submission of eleven runs.},
  booktitle={3DOR},
  url={http://diglib.eg.org/handle/10.2312/3DOR.3DOR13.073-080},
  pages={73--80},
  keywords={MINE,EN,conference,reviewed},
  year={2013}
}

@incollection{BMNK13,
  title={A state of the art report on kinect sensor setups in computer vision},
  author={Berger, Kai and Meister, Stephan and Nair, Rahul and Kondermann, Daniel},
  abstract={During the last three years after the launch of the Microsoft Kinect in the end-consumer market we have become witnesses of a small revolution in computer vision research towards the use of a standardized consumer-grade RGBD sensor for scene content retrieval. Beside classical localization and motion capturing tasks the Kinect has successfully been employed for the reconstruction of opaque and transparent objects. This report gives a comprehensive overview over the main publications using the Microsoft Kinect out of its original context as a decision-forest based motion-capturing tool.},
  booktitle={Time-of-Flight and Depth Imaging. Sensors, Algorithms, and Applications},
  pages={257--272},
  year={2013},
  url={https://goo.gl/fwVbcg},
  keywords={MINE,EN,bookchapter,reviewed},
  publisher={Springer Berlin Heidelberg}
}

@article{WBEC+14,
  title={Vehicle object retargeting from dynamic traffic videos for real-time visualisation},
  author={Walton, Simon and Berger, Kai and Ebert, David and Chen, Min},
  abstract={One form of video visualisation is to transform traffic videos from a street view to an aerial view, which facilitates a summary overview of multiple traffic video streams. This paper presents an efficient and effective solution to mitigate the undesirable distortion of the re-targeted vehicle objects in traffic video visualisation. This is achieved by a series of automated algorithmic steps, including vehicle segmentation, vehicle roof detection, and non-uniform image deformation by applying a second homography. This technique has been integrated into a video visualisation system that creates an aerial view of re-targeted video streams on top of a conventional aerial view. The results have shown that the technique offers the system a significant improvement in visual quality without undermining the requirement for real-time video visualisation.},
  journal={The Visual Computer},
  volume={30},
  number={5},
  pages={493--505},
  year={2014},
  url={https://goo.gl/qA1HeH},
  keywords={MINE,EN,journal,reviewed},
  publisher={Springer Berlin Heidelberg}
}

@article{Ber13a,
  title={The role of RGB-D benchmark datasets: an overview},
  author={Berger, Kai},
  abstract={The advent of the Microsoft Kinect three years ago stimulated not only the computer vision community for new algorithms and setups to tackle well-known problems in the community but also sparked the launch of several new benchmark datasets to which future algorithms can be compared 019 to. This review of the literature and industry developments concludes that the current RGB-D benchmark datasets can be useful to determine the accuracy of a variety of applications of a single or multiple RGB-D sensors.},
  keywords={MINE,EN,arxiv,reviewed},
  url={https://arxiv.org/pdf/1310.2053},
  journal={arXiv preprint arXiv:1310.2053},
  year={2013}
}

@article{Ber13b,
  title={A State Of the Art Report on Research in Multiple RGB-D sensor Setups},
  author={Berger, Kai},
  abstract={That the Microsoft Kinect, an RGB-D sensor, transformed the gaming and end consumer sector has been anticipated by the developers. That it also impacted in rigorous computer vision research has probably been a surprise to the whole community. Shortly before the commercial deployment of its successor, Kinect One, the research literature fills with resumees and state-of-the art papers to summarize the development over the past 3 years. This chapter describes significant research projects which have built on sensoring setups that include two or more RGB-D sensors in one scene and on RGB-D datasets captured with them which were made publicly available.},
  keywords={MINE,EN,arxiv,reviewed},
  url={https://arxiv.org/pdf/1310.2050},
  journal={arXiv preprint arXiv:1310.2050},
  year={2013}
}

@article{Ber14a,
  title={Real-time Pedestrian Surveillance with Top View Cumulative Grids},
  author={Berger, Kai and Thiyagalingam, Jeyarajan},
  abstract={This manuscript presents an efficient approach to map pedestrian surveillance footage to an aerial view for global assessment of features. The analysis of the footages relies on low level computer vision and enable real-time surveillance. While we neglect object tracking, we introduce cumulative grids on top view scene flow visualization to highlight situations of interest in the footage. Our approach is tested on multiview footage both from RGB cameras and, for the first time in the field, on RGB-D-sensors.},
  keywords={MINE,EN,arxiv,reviewed},
  journal={arXiv preprint arXiv:1402.1359},
  url={https://arxiv.org/pdf/1402.1359},
  year={2014}
}

@inproceedings{CWB+14,
  title={Visual multiplexing},
  author={Chen, Min and Walton, Simon and Berger, Kai and Thiyagalingam, Jeyarajan and Duffy, Brian and Fang, Hui and Holloway, Cameron and Trefethen, Anne E},
  abstract={The majority of display devices used in visualization are 2D displays. Inevitably, it is often necessary to overlay one piece of visual information on top of another, especially in applications such as multi-field visualization and geo-spatial information visualization. In this paper, we present a conceptual framework for studying the mechanisms for overlaying multiple pieces of visual information while allowing users to recover occluded information. We adopt the term 'multiplexing' from tele- and data communication to encompass all such overlapping mechanisms. We establish 10 categories of visual multiplexing mechanisms. We draw support evidence from both perception literature and existing works in visualization to support this conceptual framework. We examine the relationships between multiplexing and information theoretic measures. This new conceptual categorization provides the much-needed theory of visualization with an integral component.},
  booktitle={Computer Graphics Forum},
  keywords={MINE,EN,journal,reviewed},
  volume={33},
  number={3},
  pages={241--250},
  url={http://onlinelibrary.wiley.com/doi/10.1111/cgf.12380/abstract},
  year={2014}
}

@article{WBT+14a,
  title={Visualising temporal cardiovascular imagery},
  author={Walton, Simon and Berger, Kai and Thiyagalingam, Jeyan and Chen, Min and others},
  journal={Progress in Biophysics and Molecular Biology},
  keywords={MINE,EN,journal,reviewed},
  url={https://hal.inria.fr/hal-00993179},
  year={2014}
}

@incollection{BKSG14,
  title={Using sparse optical flow for two-phase gas flow capturing with multiple kinect},
  author={Berger, Kai and Kastner, Marc and Schroeder, Yannic and Guthe, Stefan},
  abstract={The use of multiple Microsoft Kinect has become prominent in the last 2 years and enjoyed widespread acceptance. While several work has been published to mitigate quality degradations in the precomputed depth image, this work focuses on employing an optical flow suitable for dot patterns as employed in the Kinect to retrieve subtle scene data alterations for reconstruction. The method is employed in a multiple Kinect vision architecture to detect the interface of propane flow around occluding objects in air.},
  booktitle={Computer Vision and Machine Learning with RGB-D Sensors},
  url={https://goo.gl/YgnFWm},
  pages={157--169},
  year={2014},
  keywords={MINE,EN,conference,reviewed},
  publisher={Springer International Publishing}
}

@incollection{Ber14b,
  title={A state of the art report on multiple RGB-D sensor research and on publicly available RGB-D datasets},
  author={Berger, Kai},
  booktitle={Computer Vision and Machine Learning with RGB-D Sensors},
  abstract={That the Microsoft Kinect, an RGB-D sensor, transformed the gaming and end consumer sector has been anticipated by the developers. That it also impacted in rigorous computer vision research has probably been a surprise to the whole community. Shortly before the commercial deployment of its successor, Kinect One, the research literature fills with resumees and state-of-the art papers to summarize the development over the past 3 years. This chapter describes significant research projects which have built on sensoring setups that include two or more RGB-D sensors in one scene and on RGB-D datasets captured with them which were made publicly available.},
  pages={27--44},
  year={2014},
  url={https://goo.gl/5JGLud},
  keywords={MINE,EN,bookcahpter,reviewed},
  publisher={Springer International Publishing}
}

@article{WBT+14b,
  title={Visualizing cardiovascular magnetic resonance (CMR) imagery: challenges and opportunities},
  author={Walton, Simon and Berger, Kai and Thiyagalingam, Jeyarajan and Duffy, Brian and Fang, Hui and Holloway, Cameron and Trefethen, Anne E and Chen, Min},
  journal={Progress in biophysics and molecular biology},
  abstract={Cardiovascular Magnetic Resonance (CMR) imaging is an essential technique for measuring regional myocardial function. However, it is a time-consuming and cognitively demanding task to interpret, identify and compare various motion characteristics based on watching CMR imagery. In this work, we focus on the problems of visualising imagery resulting from 2D myocardial tagging in CMR. In particular we provide an overview of the current state of the art of relevant visualization techniques, and a discussion on why the problem is difficult from a perceptual perspective. Finally, we introduce a proof-of-concept multilayered visualization user interface for visualizing CMR data using multiple derived attributes encoded into multivariate glyphs. An initial evaluation of the system by clinicians suggested a great potential for this visualisation technology to become a clinical practice in the future.},
  volume={115},
  number={2},
  pages={349--358},
  year={2014},
  keywords={MINE,EN,journal,reviewed},
  url={http://www.sciencedirect.com/science/article/pii/S0079610714000728},
  publisher={Pergamon}
}

@inproceedings{BRA+11,
  title={The capturing of turbulent gas flows using multiple kinects},
  author={Berger, Kai and Ruhl, Kai and Albers, Mark and Schr\"{o}der, Yannic and Scholz, Al and Kokem\"{u}ller, Jan and Guthe, Stefan and Magnor, M},
  booktitle={IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},
  abstract={We introduce the Kinect as a tool for capturing gas flows around occluders using objects of different aerodynamic properties. Previous approaches have been invasive or require elaborate setups including large printed sheets of complex noise patterns and neat lighting. Our method is easier to set up while still producing good results. We show that three Kinects are sufficient to qualitatively reconstruct nonstationary time varying gas flows in the presence of occluders.},
  pages={1108--1113},
  url={http://ieeexplore.ieee.org/document/6130374/},
  year={2011},
  keywords={MINE,EN,conference,reviewed},
  organization={IEEE}
}

@phdthesis{Ber12,
  title={Measuring, Modeling and Verification of Light-matter Interaction Phenomena},
  author={Berger, Kai},
  abstract={The photo-realistic rendering of scenes showing natural phenomena requires skilled graphic designers not only to produce a convincingly good-looking image but also to convey physical plausibility. This is especially important in industrial context, where a modelled scene showcasing a product has to approximate the actual environment of a product as closely as possible, e.g. in automotive industries. In this thesis, new techniques to measure natural phenomena are presented in order to provide new or verify existing models for rendering the physically plausible image. In contrast to other approaches, the measurement is performed using nonconventional methods: an ellipsometer is employed to capture the specular reflectance with respect to the polarisation behaviour, a transmissive screen attached to a glass tank is imaged to capture underwater reflectances, and the Microsoft Kinect, a motion capturing device, is used to detect the gas flows around objects. The results are the verification of existing, physically plausible models for commodity metals, an enhanced reflectance model for materials immersed in transparent media with known refractive index, and the reconstruction of two-phase gas flows around occluding objects.},
  keywords={MINE,EN,thesis,reviewed},
  url={https://goo.gl/TCbBPq},
  year={2012}
}



@inproceedings{BHBM14,
  title={3d volume and velocity estimation. Application to PIV tomography},
  author={Barbu, Ioana and Herzet, C\'{e}dric and Berger, Kai and M\'{e}min, Etienne},
  keywords={MINE,EN,conference,reviewed},
  booktitle={Journ{\'e}e D5 Vannes},
  year={2014}
}

@inproceedings{BBC15,
  title={UHD image reconstruction by estimating interpolation error},
  author={Berger, Kai and Berger, Kongfeng and Le Callet, Patrick},
  booktitle={IEEE International Conference on Image Processing (ICIP)},
  abstract={With the emerging ultra high definition (UHD) TV technology, the reconstruction of UHD images from transmitted HD images on the receiver's side is of great interests to save bandwidth and be compatible to existing HDTV systems. In this paper, a UHD image reconstruction algorithm is proposed for the UHDTV broadcasting system. A transmitted HD image at the receiver side is first up sampled to UHD resolution, then the error map between the interpolated UHD image and the original UHD image is estimated. The final reconstructed UHD image is the sum of the interpolated UHD image and the estimated UHD error map. There are two steps to estimate the content of the UHD error map: 1) key point detection and prediction estimate the location of the pixels with large error and create a spline tube between two adjacent key points; 2) spline-tube interpolation estimate the error value along the spline tube. Our simulation results on six images show that the proposed reconstruction method performs better than conventional up sampling without estimating the error map.},
  pages={4743--4747},
  year={2015},
  url={http://ieeexplore.ieee.org/document/7351707/},
  keywords={MINE,EN,conference,reviewed},
  organization={IEEE}
}

@article{RBL+14,
  title={Caroline: An Autonomously Driving Vehicle for Urban Environments},
  author={Rauskolb, Fred W and Berger, Kai and Lipski, Christian and Magnor, Marcus and Cornelsen, Karsten and Effertz, Jan and Form, Thomas and Graefe, Fabian and Ohl, Sebastian and Schumacher, Walter and others},
  journal={arXiv preprint arXiv:1409.6584},
  keywords={MINE,EN,arxiv,reviewed},
  year={2014}
}

@incollection{RBL+09,
  title={Caroline: An autonomously driving vehicle for urban environments},
  author={Rauskolb, Fred W and Berger, Kai and Lipski, Christian and Magnor, Marcus and Cornelsen, Karsten and Effertz, Jan and Form, Thomas and Graefe, Fabian and Ohl, Sebastian and Schumacher, Walter and others},
  booktitle={The DARPA Urban Challenge},
  pages={441--508},
  year={2009},
  keywords={MINE,EN,bookchapter,reviewed},
  publisher={Springer Berlin Heidelberg}
}

@inproceedings{BVM17,
  title={Depth from stereo polarization in specular scenes for urban robotics},
  author={Berger, Kai and Voorhies, Randolph and Matthies, Larry H},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  abstract={3-D perception of scenes with specular surfaces is still challenging for robotics applications in urban areas, for both active and passive range sensors; there is a need for improved solutions that work without artificial illumination over a wide range of distances. The advent of cameras with microgrid polarization filter arrays, which allow acquiring four orientations of linearly polarized images simultaneously, has potential to make the use of polarization information in 3-D perception more practical. It is well-known that polarization can provide information about the orientation of specular surfaces; however, prior work with polarization for 3-D perception has had several limitations. We present the first unified formulation of depth perception with stereo and polarization by extending previous energy minimization formulations to include surface orientation constraints computed from the polarization channels. We apply an existing quadratic pseudo-boolean optimization (QPBO) method to approximate the optimal depth map. We use synthetic and real indoor/outdoor images to demonstrate that the new method achieves better results than prior methods, with fewer assumptions and limitations.},
  pages={1966--1973},
  year={2017},
  url={http://ieeexplore.ieee.org/document/7989227/},
  keywords={MINE,EN,conference,reviewed},
  organization={IEEE}
}

@inproceedings{BVM16,
  title={Incorporating polarization in stereo vision-based 3D perception of non-Lambertian scenes},
  author={Berger, Kai and Voorhies, Randolph and Matthies, Larry},
  abstract={Surfaces with specular, non-Lambertian reflectance are common in urban areas. Robot perception systems for applications in urban environments need to function effectively in the presence of such materials; however, both passive and active 3-D perception systems have difficulties with them. In this paper, we develop an approach using a stereo pair of polarization cameras to improve passive 3-D perception of specular surfaces. We use a commercial stereo camera pair with rotatable polarization filters in front of each lens to capture images with multiple orientations of the polarization filter. From these images, we estimate the degree of linear polarization (DOLP) and the angle of polarization (AOP) at each pixel in at least one camera. The AOP constrains the corresponding surface normal in the scene to lie in the plane of the observed angle of polarization. We embody this constraint an energy functional for a regularization-based stereo vision algorithm. This paper describes the theory of polarization needed for this approach, describes the new stereo vision algorithm, and presents results on synthetic and real images to evaluate performance.},
  booktitle={SPIE Defense+ Security},
  pages={98370P--98370P},
  year={2016},
  url={https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9837/1/Incorporating-polarization-in-stereo-vision-based-3D-perception-of-non/10.1117/12.2231110.short?SSO=1},
  keywords={MINE,EN,conference,reviewed},
  organization={International Society for Optics and Photonics}
}







